{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99b17f9e-cbf5-468e-98a1-5863fa454c95",
   "metadata": {},
   "source": [
    "# Execute below to run local llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b0f1022-4d15-4781-8ea4-e1172b4dd6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "chmod +x local_llm_setup.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af98e3e8-023d-4865-a107-8cb1cfa40770",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vllm in ./vllm_env/lib/python3.11/site-packages (0.11.0)\n",
      "Requirement already satisfied: regex in ./vllm_env/lib/python3.11/site-packages (from vllm) (2025.9.18)\n",
      "Requirement already satisfied: cachetools in ./vllm_env/lib/python3.11/site-packages (from vllm) (6.2.0)\n",
      "Requirement already satisfied: psutil in ./vllm_env/lib/python3.11/site-packages (from vllm) (7.1.0)\n",
      "Requirement already satisfied: sentencepiece in ./vllm_env/lib/python3.11/site-packages (from vllm) (0.2.1)\n",
      "Requirement already satisfied: numpy in ./vllm_env/lib/python3.11/site-packages (from vllm) (2.2.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in ./vllm_env/lib/python3.11/site-packages (from vllm) (2.32.5)\n",
      "Requirement already satisfied: tqdm in ./vllm_env/lib/python3.11/site-packages (from vllm) (4.67.1)\n",
      "Requirement already satisfied: blake3 in ./vllm_env/lib/python3.11/site-packages (from vllm) (1.0.7)\n",
      "Requirement already satisfied: py-cpuinfo in ./vllm_env/lib/python3.11/site-packages (from vllm) (9.0.0)\n",
      "Requirement already satisfied: transformers>=4.55.2 in ./vllm_env/lib/python3.11/site-packages (from vllm) (4.57.0)\n",
      "Requirement already satisfied: tokenizers>=0.21.1 in ./vllm_env/lib/python3.11/site-packages (from vllm) (0.22.1)\n",
      "Requirement already satisfied: protobuf in ./vllm_env/lib/python3.11/site-packages (from vllm) (6.32.1)\n",
      "Requirement already satisfied: fastapi>=0.115.0 in ./vllm_env/lib/python3.11/site-packages (from fastapi[standard]>=0.115.0->vllm) (0.118.0)\n",
      "Requirement already satisfied: aiohttp in ./vllm_env/lib/python3.11/site-packages (from vllm) (3.12.15)\n",
      "Requirement already satisfied: openai>=1.99.1 in ./vllm_env/lib/python3.11/site-packages (from vllm) (2.1.0)\n",
      "Requirement already satisfied: pydantic>=2.11.7 in ./vllm_env/lib/python3.11/site-packages (from vllm) (2.11.10)\n",
      "Requirement already satisfied: prometheus_client>=0.18.0 in ./vllm_env/lib/python3.11/site-packages (from vllm) (0.23.1)\n",
      "Requirement already satisfied: pillow in ./vllm_env/lib/python3.11/site-packages (from vllm) (11.3.0)\n",
      "Requirement already satisfied: prometheus-fastapi-instrumentator>=7.0.0 in ./vllm_env/lib/python3.11/site-packages (from vllm) (7.1.0)\n",
      "Requirement already satisfied: tiktoken>=0.6.0 in ./vllm_env/lib/python3.11/site-packages (from vllm) (0.11.0)\n",
      "Requirement already satisfied: lm-format-enforcer==0.11.3 in ./vllm_env/lib/python3.11/site-packages (from vllm) (0.11.3)\n",
      "Requirement already satisfied: llguidance<0.8.0,>=0.7.11 in ./vllm_env/lib/python3.11/site-packages (from vllm) (0.7.30)\n",
      "Requirement already satisfied: outlines_core==0.2.11 in ./vllm_env/lib/python3.11/site-packages (from vllm) (0.2.11)\n",
      "Requirement already satisfied: diskcache==5.6.3 in ./vllm_env/lib/python3.11/site-packages (from vllm) (5.6.3)\n",
      "Requirement already satisfied: lark==1.2.2 in ./vllm_env/lib/python3.11/site-packages (from vllm) (1.2.2)\n",
      "Requirement already satisfied: xgrammar==0.1.25 in ./vllm_env/lib/python3.11/site-packages (from vllm) (0.1.25)\n",
      "Requirement already satisfied: typing_extensions>=4.10 in ./vllm_env/lib/python3.11/site-packages (from vllm) (4.15.0)\n",
      "Requirement already satisfied: filelock>=3.16.1 in ./vllm_env/lib/python3.11/site-packages (from vllm) (3.19.1)\n",
      "Requirement already satisfied: partial-json-parser in ./vllm_env/lib/python3.11/site-packages (from vllm) (0.2.1.1.post6)\n",
      "Requirement already satisfied: pyzmq>=25.0.0 in ./vllm_env/lib/python3.11/site-packages (from vllm) (27.1.0)\n",
      "Requirement already satisfied: msgspec in ./vllm_env/lib/python3.11/site-packages (from vllm) (0.19.0)\n",
      "Requirement already satisfied: gguf>=0.13.0 in ./vllm_env/lib/python3.11/site-packages (from vllm) (0.17.1)\n",
      "Requirement already satisfied: mistral_common>=1.8.2 in ./vllm_env/lib/python3.11/site-packages (from mistral_common[audio,image]>=1.8.2->vllm) (1.8.5)\n",
      "Requirement already satisfied: opencv-python-headless>=4.11.0 in ./vllm_env/lib/python3.11/site-packages (from vllm) (4.12.0.88)\n",
      "Requirement already satisfied: pyyaml in ./vllm_env/lib/python3.11/site-packages (from vllm) (6.0.3)\n",
      "Requirement already satisfied: einops in ./vllm_env/lib/python3.11/site-packages (from vllm) (0.8.1)\n",
      "Requirement already satisfied: compressed-tensors==0.11.0 in ./vllm_env/lib/python3.11/site-packages (from vllm) (0.11.0)\n",
      "Requirement already satisfied: depyf==0.19.0 in ./vllm_env/lib/python3.11/site-packages (from vllm) (0.19.0)\n",
      "Requirement already satisfied: cloudpickle in ./vllm_env/lib/python3.11/site-packages (from vllm) (3.1.1)\n",
      "Requirement already satisfied: watchfiles in ./vllm_env/lib/python3.11/site-packages (from vllm) (1.1.0)\n",
      "Requirement already satisfied: python-json-logger in ./vllm_env/lib/python3.11/site-packages (from vllm) (3.3.0)\n",
      "Requirement already satisfied: scipy in ./vllm_env/lib/python3.11/site-packages (from vllm) (1.16.2)\n",
      "Requirement already satisfied: ninja in ./vllm_env/lib/python3.11/site-packages (from vllm) (1.13.0)\n",
      "Requirement already satisfied: pybase64 in ./vllm_env/lib/python3.11/site-packages (from vllm) (1.4.2)\n",
      "Requirement already satisfied: cbor2 in ./vllm_env/lib/python3.11/site-packages (from vllm) (5.7.0)\n",
      "Requirement already satisfied: setproctitle in ./vllm_env/lib/python3.11/site-packages (from vllm) (1.3.7)\n",
      "Requirement already satisfied: openai-harmony>=0.0.3 in ./vllm_env/lib/python3.11/site-packages (from vllm) (0.0.4)\n",
      "Requirement already satisfied: numba==0.61.2 in ./vllm_env/lib/python3.11/site-packages (from vllm) (0.61.2)\n",
      "Requirement already satisfied: ray>=2.48.0 in ./vllm_env/lib/python3.11/site-packages (from ray[cgraph]>=2.48.0->vllm) (2.49.2)\n",
      "Requirement already satisfied: torch==2.8.0 in ./vllm_env/lib/python3.11/site-packages (from vllm) (2.8.0)\n",
      "Requirement already satisfied: torchaudio==2.8.0 in ./vllm_env/lib/python3.11/site-packages (from vllm) (2.8.0)\n",
      "Requirement already satisfied: torchvision==0.23.0 in ./vllm_env/lib/python3.11/site-packages (from vllm) (0.23.0)\n",
      "Requirement already satisfied: xformers==0.0.32.post1 in ./vllm_env/lib/python3.11/site-packages (from vllm) (0.0.32.post1)\n",
      "Requirement already satisfied: frozendict in ./vllm_env/lib/python3.11/site-packages (from compressed-tensors==0.11.0->vllm) (2.4.6)\n",
      "Requirement already satisfied: astor in ./vllm_env/lib/python3.11/site-packages (from depyf==0.19.0->vllm) (0.8.1)\n",
      "Requirement already satisfied: dill in ./vllm_env/lib/python3.11/site-packages (from depyf==0.19.0->vllm) (0.4.0)\n",
      "Requirement already satisfied: interegular>=0.3.2 in ./vllm_env/lib/python3.11/site-packages (from lm-format-enforcer==0.11.3->vllm) (0.3.3)\n",
      "Requirement already satisfied: packaging in ./vllm_env/lib/python3.11/site-packages (from lm-format-enforcer==0.11.3->vllm) (25.0)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in ./vllm_env/lib/python3.11/site-packages (from numba==0.61.2->vllm) (0.44.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./vllm_env/lib/python3.11/site-packages (from torch==2.8.0->vllm) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./vllm_env/lib/python3.11/site-packages (from torch==2.8.0->vllm) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./vllm_env/lib/python3.11/site-packages (from torch==2.8.0->vllm) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./vllm_env/lib/python3.11/site-packages (from torch==2.8.0->vllm) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./vllm_env/lib/python3.11/site-packages (from torch==2.8.0->vllm) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./vllm_env/lib/python3.11/site-packages (from torch==2.8.0->vllm) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./vllm_env/lib/python3.11/site-packages (from torch==2.8.0->vllm) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./vllm_env/lib/python3.11/site-packages (from torch==2.8.0->vllm) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./vllm_env/lib/python3.11/site-packages (from torch==2.8.0->vllm) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./vllm_env/lib/python3.11/site-packages (from torch==2.8.0->vllm) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./vllm_env/lib/python3.11/site-packages (from torch==2.8.0->vllm) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./vllm_env/lib/python3.11/site-packages (from torch==2.8.0->vllm) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./vllm_env/lib/python3.11/site-packages (from torch==2.8.0->vllm) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./vllm_env/lib/python3.11/site-packages (from torch==2.8.0->vllm) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in ./vllm_env/lib/python3.11/site-packages (from torch==2.8.0->vllm) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./vllm_env/lib/python3.11/site-packages (from torch==2.8.0->vllm) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./vllm_env/lib/python3.11/site-packages (from torch==2.8.0->vllm) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./vllm_env/lib/python3.11/site-packages (from torch==2.8.0->vllm) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in ./vllm_env/lib/python3.11/site-packages (from torch==2.8.0->vllm) (3.4.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in ./vllm_env/lib/python3.11/site-packages (from triton==3.4.0->torch==2.8.0->vllm) (65.5.0)\n",
      "Requirement already satisfied: starlette<0.49.0,>=0.40.0 in ./vllm_env/lib/python3.11/site-packages (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm) (0.48.0)\n",
      "Requirement already satisfied: fastapi-cli>=0.0.8 in ./vllm_env/lib/python3.11/site-packages (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.0.13)\n",
      "Requirement already satisfied: httpx<1.0.0,>=0.23.0 in ./vllm_env/lib/python3.11/site-packages (from fastapi[standard]>=0.115.0->vllm) (0.28.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in ./vllm_env/lib/python3.11/site-packages (from fastapi[standard]>=0.115.0->vllm) (0.0.20)\n",
      "Requirement already satisfied: email-validator>=2.0.0 in ./vllm_env/lib/python3.11/site-packages (from fastapi[standard]>=0.115.0->vllm) (2.3.0)\n",
      "Requirement already satisfied: uvicorn>=0.12.0 in ./vllm_env/lib/python3.11/site-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.37.0)\n",
      "Requirement already satisfied: jsonschema>=4.21.1 in ./vllm_env/lib/python3.11/site-packages (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (4.25.1)\n",
      "Requirement already satisfied: pydantic-extra-types>=2.10.5 in ./vllm_env/lib/python3.11/site-packages (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (2.10.5)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./vllm_env/lib/python3.11/site-packages (from openai>=1.99.1->vllm) (4.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./vllm_env/lib/python3.11/site-packages (from openai>=1.99.1->vllm) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./vllm_env/lib/python3.11/site-packages (from openai>=1.99.1->vllm) (0.11.0)\n",
      "Requirement already satisfied: sniffio in ./vllm_env/lib/python3.11/site-packages (from openai>=1.99.1->vllm) (1.3.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./vllm_env/lib/python3.11/site-packages (from pydantic>=2.11.7->vllm) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./vllm_env/lib/python3.11/site-packages (from pydantic>=2.11.7->vllm) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./vllm_env/lib/python3.11/site-packages (from pydantic>=2.11.7->vllm) (0.4.2)\n",
      "Requirement already satisfied: click>=7.0 in ./vllm_env/lib/python3.11/site-packages (from ray>=2.48.0->ray[cgraph]>=2.48.0->vllm) (8.3.0)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in ./vllm_env/lib/python3.11/site-packages (from ray>=2.48.0->ray[cgraph]>=2.48.0->vllm) (1.1.1)\n",
      "Requirement already satisfied: cupy-cuda12x in ./vllm_env/lib/python3.11/site-packages (from ray[cgraph]>=2.48.0->vllm) (13.6.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./vllm_env/lib/python3.11/site-packages (from requests>=2.26.0->vllm) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./vllm_env/lib/python3.11/site-packages (from requests>=2.26.0->vllm) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./vllm_env/lib/python3.11/site-packages (from requests>=2.26.0->vllm) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./vllm_env/lib/python3.11/site-packages (from requests>=2.26.0->vllm) (2025.8.3)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in ./vllm_env/lib/python3.11/site-packages (from tokenizers>=0.21.1->vllm) (0.35.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./vllm_env/lib/python3.11/site-packages (from transformers>=4.55.2->vllm) (0.6.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./vllm_env/lib/python3.11/site-packages (from aiohttp->vllm) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./vllm_env/lib/python3.11/site-packages (from aiohttp->vllm) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./vllm_env/lib/python3.11/site-packages (from aiohttp->vllm) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./vllm_env/lib/python3.11/site-packages (from aiohttp->vllm) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./vllm_env/lib/python3.11/site-packages (from aiohttp->vllm) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./vllm_env/lib/python3.11/site-packages (from aiohttp->vllm) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./vllm_env/lib/python3.11/site-packages (from aiohttp->vllm) (1.20.1)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in ./vllm_env/lib/python3.11/site-packages (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm) (2.8.0)\n",
      "Requirement already satisfied: typer>=0.15.1 in ./vllm_env/lib/python3.11/site-packages (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.19.2)\n",
      "Requirement already satisfied: rich-toolkit>=0.14.8 in ./vllm_env/lib/python3.11/site-packages (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.15.1)\n",
      "Requirement already satisfied: fastapi-cloud-cli>=0.1.1 in ./vllm_env/lib/python3.11/site-packages (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.3.0)\n",
      "Requirement already satisfied: httpcore==1.* in ./vllm_env/lib/python3.11/site-packages (from httpx<1.0.0,>=0.23.0->fastapi[standard]>=0.115.0->vllm) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./vllm_env/lib/python3.11/site-packages (from httpcore==1.*->httpx<1.0.0,>=0.23.0->fastapi[standard]>=0.115.0->vllm) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./vllm_env/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.21.1->vllm) (1.1.10)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./vllm_env/lib/python3.11/site-packages (from jinja2->torch==2.8.0->vllm) (3.0.3)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./vllm_env/lib/python3.11/site-packages (from jsonschema>=4.21.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in ./vllm_env/lib/python3.11/site-packages (from jsonschema>=4.21.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in ./vllm_env/lib/python3.11/site-packages (from jsonschema>=4.21.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (0.27.1)\n",
      "Requirement already satisfied: pycountry>=23 in ./vllm_env/lib/python3.11/site-packages (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (24.6.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./vllm_env/lib/python3.11/site-packages (from sympy>=1.13.3->torch==2.8.0->vllm) (1.3.0)\n",
      "Requirement already satisfied: httptools>=0.6.3 in ./vllm_env/lib/python3.11/site-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in ./vllm_env/lib/python3.11/site-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (1.1.1)\n",
      "Requirement already satisfied: uvloop>=0.15.1 in ./vllm_env/lib/python3.11/site-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.21.0)\n",
      "Requirement already satisfied: websockets>=10.4 in ./vllm_env/lib/python3.11/site-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (15.0.1)\n",
      "Requirement already satisfied: fastrlock>=0.5 in ./vllm_env/lib/python3.11/site-packages (from cupy-cuda12x->ray[cgraph]>=2.48.0->vllm) (0.8.3)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in ./vllm_env/lib/python3.11/site-packages (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (0.13.1)\n",
      "Requirement already satisfied: soxr>=0.5.0 in ./vllm_env/lib/python3.11/site-packages (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (1.0.0)\n",
      "Requirement already satisfied: rignore>=0.5.1 in ./vllm_env/lib/python3.11/site-packages (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.7.0)\n",
      "Requirement already satisfied: sentry-sdk>=2.20.0 in ./vllm_env/lib/python3.11/site-packages (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (2.39.0)\n",
      "Requirement already satisfied: rich>=13.7.1 in ./vllm_env/lib/python3.11/site-packages (from rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (14.1.0)\n",
      "Requirement already satisfied: cffi>=1.0 in ./vllm_env/lib/python3.11/site-packages (from soundfile>=0.12.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (2.0.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in ./vllm_env/lib/python3.11/site-packages (from typer>=0.15.1->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (1.5.4)\n",
      "Requirement already satisfied: pycparser in ./vllm_env/lib/python3.11/site-packages (from cffi>=1.0->soundfile>=0.12.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (2.23)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./vllm_env/lib/python3.11/site-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./vllm_env/lib/python3.11/site-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./vllm_env/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-04 19:03:11 [__init__.py:216] Automatically detected platform cuda.\n",
      "\u001b[1;36m(APIServer pid=27911)\u001b[0;0m INFO 10-04 19:03:14 [api_server.py:1839] vLLM API server version 0.11.0\n",
      "\u001b[1;36m(APIServer pid=27911)\u001b[0;0m INFO 10-04 19:03:14 [utils.py:233] non-default args: {'model_tag': 'NousResearch/Meta-Llama-3-8B-Instruct', 'port': 9000, 'api_key': ['token-abc123'], 'model': 'NousResearch/Meta-Llama-3-8B-Instruct'}\n",
      "\u001b[1;36m(APIServer pid=27911)\u001b[0;0m INFO 10-04 19:03:16 [model.py:547] Resolved architecture: LlamaForCausalLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(APIServer pid=27911)\u001b[0;0m `torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(APIServer pid=27911)\u001b[0;0m INFO 10-04 19:03:16 [model.py:1510] Using max model len 8192\n",
      "\u001b[1;36m(APIServer pid=27911)\u001b[0;0m INFO 10-04 19:03:16 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 10-04 19:03:19 [__init__.py:216] Automatically detected platform cuda.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=27974)\u001b[0;0m INFO 10-04 19:03:22 [core.py:644] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=27974)\u001b[0;0m INFO 10-04 19:03:22 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='NousResearch/Meta-Llama-3-8B-Instruct', speculative_config=None, tokenizer='NousResearch/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=NousResearch/Meta-Llama-3-8B-Instruct, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\",\"vllm.sparse_attn_indexer\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=27974)\u001b[0;0m INFO 10-04 19:03:23 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=27974)\u001b[0;0m WARNING 10-04 19:03:23 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=27974)\u001b[0;0m INFO 10-04 19:03:23 [gpu_model_runner.py:2602] Starting to load model NousResearch/Meta-Llama-3-8B-Instruct...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=27974)\u001b[0;0m INFO 10-04 19:03:24 [gpu_model_runner.py:2634] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=27974)\u001b[0;0m INFO 10-04 19:03:24 [cuda.py:366] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=27974)\u001b[0;0m INFO 10-04 19:03:25 [weight_utils.py:392] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:00,  6.32it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  2.14it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.67it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.52it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.71it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=27974)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=27974)\u001b[0;0m INFO 10-04 19:03:28 [default_loader.py:267] Loading weights took 2.41 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=27974)\u001b[0;0m INFO 10-04 19:03:28 [gpu_model_runner.py:2653] Model loading took 14.9596 GiB and 4.217537 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=27974)\u001b[0;0m INFO 10-04 19:03:32 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/c640701a30/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=27974)\u001b[0;0m INFO 10-04 19:03:32 [backends.py:559] Dynamo bytecode transform time: 3.73 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=27974)\u001b[0;0m INFO 10-04 19:03:34 [backends.py:164] Directly load the compiled graph(s) for dynamic shape from the cache, took 1.495 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=27974)\u001b[0;0m INFO 10-04 19:03:34 [monitor.py:34] torch.compile takes 3.73 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=27974)\u001b[0;0m INFO 10-04 19:03:35 [gpu_worker.py:298] Available KV cache memory: 51.33 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=27974)\u001b[0;0m INFO 10-04 19:03:35 [kv_cache_utils.py:1087] GPU KV cache size: 420,512 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=27974)\u001b[0;0m INFO 10-04 19:03:35 [kv_cache_utils.py:1091] Maximum concurrency for 8,192 tokens per request: 51.33x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:02<00:00, 30.23it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 67/67 [00:01<00:00, 39.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=27974)\u001b[0;0m INFO 10-04 19:03:39 [gpu_model_runner.py:3480] Graph capturing finished in 4 secs, took 1.19 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=27974)\u001b[0;0m INFO 10-04 19:03:39 [core.py:210] init engine (profile, create kv cache, warmup model) took 11.36 seconds\n",
      "\u001b[1;36m(APIServer pid=27911)\u001b[0;0m INFO 10-04 19:03:41 [loggers.py:147] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 26282\n",
      "\u001b[1;36m(APIServer pid=27911)\u001b[0;0m INFO 10-04 19:03:41 [api_server.py:1634] Supported_tasks: ['generate']\n",
      "\u001b[1;36m(APIServer pid=27911)\u001b[0;0m WARNING 10-04 19:03:41 [model.py:1389] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.\n",
      "\u001b[1;36m(APIServer pid=27911)\u001b[0;0m INFO 10-04 19:03:41 [serving_responses.py:137] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}\n",
      "\u001b[1;36m(APIServer pid=27911)\u001b[0;0m INFO 10-04 19:03:41 [serving_chat.py:139] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}\n",
      "\u001b[1;36m(APIServer pid=27911)\u001b[0;0m INFO 10-04 19:03:41 [serving_completion.py:76] Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.9}\n",
      "\u001b[1;36m(APIServer pid=27911)\u001b[0;0m INFO 10-04 19:03:41 [api_server.py:1912] Starting vLLM API server 0 on http://0.0.0.0:9000\n",
      "\u001b[1;36m(APIServer pid=27911)\u001b[0;0m INFO 10-04 19:03:41 [launcher.py:34] Available routes are:\n",
      "\u001b[1;36m(APIServer pid=27911)\u001b[0;0m INFO 10-04 19:03:41 [launcher.py:42] Route: /openapi.json, Methods: HEAD, GET\n",
      "\u001b[1;36m(APIServer pid=27911)\u001b[0;0m INFO 10-04 19:03:41 [launcher.py:42] Route: /docs, Methods: HEAD, GET\n",
      "\u001b[1;36m(APIServer pid=27911)\u001b[0;0m INFO 10-04 19:03:41 [launcher.py:42] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
      "\u001b[1;36m(APIServer pid=27911)\u001b[0;0m INFO 10-04 19:03:41 [launcher.py:42] Route: /redoc, Methods: HEAD, GET\n",
      "\u001b[1;36m(APIServer pid=27911)\u001b[0;0m INFO 10-04 19:03:41 [launcher.py:42] Route: /health, Methods: GET\n",
      "\u001b[1;36m(APIServer pid=27911)\u001b[0;0m INFO 10-04 19:03:41 [launcher.py:42] Route: /load, Methods: GET\n",
      "\u001b[1;36m(APIServer pid=27911)\u001b[0;0m INFO 10-04 19:03:41 [launcher.py:42] Route: /ping, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=27911)\u001b[0;0m INFO 10-04 19:03:41 [launcher.py:42] Route: /ping, Methods: GET\n",
      "\u001b[1;36m(APIServer pid=27911)\u001b[0;0m INFO 10-04 19:03:41 [launcher.py:42] Route: /tokenize, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=27911)\u001b[0;0m INFO 10-04 19:03:41 [launcher.py:42] Route: /detokenize, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=27911)\u001b[0;0m INFO 10-04 19:03:41 [launcher.py:42] Route: /v1/models, Methods: GET\n",
      "\u001b[1;36m(APIServer pid=27911)\u001b[0;0m INFO 10-04 19:03:41 [launcher.py:42] Route: /version, Methods: GET\n",
      "\u001b[1;36m(APIServer pid=27911)\u001b[0;0m INFO 10-04 19:03:41 [launcher.py:42] Route: /v1/responses, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=27911)\u001b[0;0m INFO 10-04 19:03:41 [launcher.py:42] Route: /v1/responses/{response_id}, Methods: GET\n",
      "\u001b[1;36m(APIServer pid=27911)\u001b[0;0m INFO 10-04 19:03:41 [launcher.py:42] Route: /v1/responses/{response_id}/cancel, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=27911)\u001b[0;0m INFO 10-04 19:03:41 [launcher.py:42] Route: /v1/chat/completions, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=27911)\u001b[0;0m INFO 10-04 19:03:41 [launcher.py:42] Route: /v1/completions, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=27911)\u001b[0;0m INFO 10-04 19:03:41 [launcher.py:42] Route: /v1/embeddings, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=27911)\u001b[0;0m INFO 10-04 19:03:41 [launcher.py:42] Route: /pooling, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=27911)\u001b[0;0m INFO 10-04 19:03:41 [launcher.py:42] Route: /classify, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=27911)\u001b[0;0m INFO 10-04 19:03:41 [launcher.py:42] Route: /score, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=27911)\u001b[0;0m INFO 10-04 19:03:41 [launcher.py:42] Route: /v1/score, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=27911)\u001b[0;0m INFO 10-04 19:03:41 [launcher.py:42] Route: /v1/audio/transcriptions, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=27911)\u001b[0;0m INFO 10-04 19:03:41 [launcher.py:42] Route: /v1/audio/translations, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=27911)\u001b[0;0m INFO 10-04 19:03:41 [launcher.py:42] Route: /rerank, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=27911)\u001b[0;0m INFO 10-04 19:03:41 [launcher.py:42] Route: /v1/rerank, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=27911)\u001b[0;0m INFO 10-04 19:03:41 [launcher.py:42] Route: /v2/rerank, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=27911)\u001b[0;0m INFO 10-04 19:03:41 [launcher.py:42] Route: /scale_elastic_ep, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=27911)\u001b[0;0m INFO 10-04 19:03:41 [launcher.py:42] Route: /is_scaling_elastic_ep, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=27911)\u001b[0;0m INFO 10-04 19:03:41 [launcher.py:42] Route: /invocations, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=27911)\u001b[0;0m INFO 10-04 19:03:41 [launcher.py:42] Route: /metrics, Methods: GET\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(APIServer pid=27911)\u001b[0;0m INFO:     Started server process [27911]\n",
      "\u001b[1;36m(APIServer pid=27911)\u001b[0;0m INFO:     Waiting for application startup.\n",
      "\u001b[1;36m(APIServer pid=27911)\u001b[0;0m INFO:     Application startup complete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(APIServer pid=27911)\u001b[0;0m INFO 10-04 19:03:52 [chat_utils.py:560] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n",
      "\u001b[1;36m(APIServer pid=27911)\u001b[0;0m INFO:     127.0.0.1:47948 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[1;36m(APIServer pid=27911)\u001b[0;0m INFO:     127.0.0.1:46696 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "./local_llm_setup.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bc01f9-f3d3-4de0-a9bc-949116427ef3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
